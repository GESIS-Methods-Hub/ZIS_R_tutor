---
title: "Basic Analysis for Scale Archiving"
subtitle: "*ZIS R-Tutorials*"
logo: pics/zis_logo.svg
bibliography: basic.bib

author:
  - name: Julian Urban
    affiliation: "GESIS – Leibniz Institute for the Social Sciences"
  - name: David Grüning
    affiliation: ""
  - name: Piotr Koc
    affiliation: ""

   
date: 2024-07-01
date-format: "DD.MM.YYYY"
date-modified	: last-modified
format:
  html:
    code-overflow: wrap
    code-copy: true
    toc: true
    toc-location: left
    theme: cosmo
    html-math-method: katex
    css: technical/styles.css
    template-partials:
       - technical/title-block.html
    
license: "CC BY-NC 4.0"
citation: 
    type: "document"
    title: |
        Basic Analysis for Scale Archiving
    author:
       - name: Julian Urban
       - name: David Grüning
       - name: Piotr Koc
    issued: 2024
    container-title: ZIS R-tutorials
    publisher: GESIS – Leibniz Institute for the Social Sciences 
    URL: https://zis.gesis.org/infotexte/GuidelineMaterials.html
    
---
## Introduction

The following tutorial presents the basic analysis required for an instrument to be archived in the Open Access Repository for Measurement Instruments (ZIS). As an example, we use a scale with continuous indicators, so not all parts of the analysis are appropriate for instruments with categorical indicators. We point this out whenever relevant. The treatment of categorical indicators is covered in a separate tutorial. 

## Data Preparation & Descriptive Analysis
In this tutorial, we will use the `HolzingerSwineford1939` data set, which contains mental ability test scores of seventh- and eighth-grade children from two different schools (Pasteur and Grant-White). 

```{r}
#| message: false
library(lavaan)
library(semTools)
library(psych)
library(Hmisc)
library(dplyr)

# Loading the data
Data <- lavaan::HolzingerSwineford1939
```

First, let's compute the descriptives for the nominal variables --- the counts. You can compute the counts using the `table()` function.

```{r}
# Count for a single variable
table(Data$sex,
      useNA = "always")
```


```{r}
# Define nominal variables
nominal_variables <- c("sex", "school", "grade")

# Count for multiple variables
nominal_sample_statistics <- list()
for (variable in nominal_variables) {
  frequency_table <- table(Data[, variable],
                           useNA = "always")
  nominal_sample_statistics[[variable]] <- frequency_table
}
nominal_sample_statistics

```
If you have multiple variables, you can use a loop that iterates over the elements of the `nominal_variables` vector and produces as many tables with counts as there are variables in that vector. 


For continuous variables, we compute the mean, standard deviation, skewness, kurtosis, and the percentage of missing data.

```{r}
# Compute aggregated age variable of year and months
Data$age <- (Data$ageyr * 12 + Data$agemo) / 12

# Estimate mean, sd, skew, kurtosis, and percentage missing
avrg_ <- mean(Data$age, na.rm = TRUE)
sd_ <- sd(Data$age, na.rm = TRUE)
skw_ <-psych::skew(Data$age, na.rm = TRUE)
krtss_<- psych::kurtosi(Data$age, na.rm = TRUE)
mis_ <- sum(is.na(Data$age)) / nrow(Data)

smmry <- data.frame(
  var = "age",
  avrg_ = avrg_,
  sd_ = sd_,
  skw_ = skw_,
  krtss_ = krtss_,
  mis_ = mis_
)

# Format the numeric columns to 2 decimal places using sprintf
smmry <- smmry %>%
  mutate(across(c(avrg_, sd_, skw_, krtss_), 
                ~ sprintf("%.2f", .)))

# Rename columns
colnames(smmry) <- c("Variable", "Mean", "Standard Deviation", "Skewness", "Kurtosis", "% Missing Data")
smmry
```
Alternatively, we could use the `describe()` function from the `psych` package, which can be particularly useful with multiple continuous variables. The only caveat of that function is that it will not produce the percentage of missing observation per variable automatically, which we could easily circumvent by running `colSums(is.na())/n`, where `n` is the total number of observations.  

## Dimensionality Assessment & Factorial Validity 
First, we build a look-up table where items are assigned to different subscales. We will use the table for the subsequent analyses in this section. 

```{r}
# Build lookup table for required items
lookup_table <- data.frame(item = paste("x", c(1:9), sep = ""),
                           subscale = c(rep(c("visual", "textual", "speed"),
                                            each = 3))
                           )
```
### Exploratory Factor Analysis 
To decide how many latent dimension to extract, we will first use parallel analysis (for the factor solution, not principle components) and exploratory factor analysis. We run parallel analysis using the following syntax: 

```{r}
#| message: false
#| label: fig-figpar
set.seed(8576456)
# Conduct parallel analysis display screeplot
parallel_analyses_efa <- psych::fa.parallel(Data[ , unlist(lookup_table$item)],
                                            fa = "fa", # Method to explore dimensionality: efa
                                            fm = "minres", nfactors = 1, SMC = F,
                                            n.iter = 30, # quant = .95,
                                            cor = "cor")                            
```
::: {.column-margin}
For categorical data, you would need to change the `cor =` argument in the `fa.parallel()` function to `"poly"` and use polychoric correlations.  
:::

The idea behind parallel analysis in exploratory factor analysis (EFA) involves comparing the eigenvalues from your actual data to those from randomly generated data to determine the number of factors to retain. The random datasets match your actual dataset in terms of sample size and number of variables. Eigenvalues are calculated for both the real data and the random data. This process is repeated multiple times to generate a distribution of eigenvalues for the random data. 

The key step is then to compare the eigenvalues from your actual data with the mean (or sometimes the 95th percentile) of the eigenvalues from the random datasets. You retain those factors where the actual eigenvalue exceeds the corresponding eigenvalue from the random data. In our case,  @fig-figpar with a scree plot suggests that we should extract three factors. 

::: {.callout-important appearance="simple"}
## Be careful
Quite often researchers use the Kaiser-1 rule to decide the number of latent factors to extract. That is, they check how many eigenvalues are greater than 1. This method has been shown not be robust and result in extracting too many latent dimensions [see, @russell_search_2002; @van_der_eijk_risky_2015]. Hence, you should probably refrain from using it. 
:::

Knowing how many factors to extract, we will estimate now an EFA model with three factors using the oblique rotation and minimal residuals (a.k.a. ordinary least squares) as the extraction method^[If you are interested in the details of exploratory factor analysis, you might want to check the book by @fabrigar_exploratory_2012]. 

```{r}
# Estimate exploratory factor analyses
efa <- psych::fa(Data[ , unlist(lookup_table$item)],
                 fm = "pa",
                 nfactors = 3,
                 rotate = "oblimin",
                 scores = "regression", oblique.scores = F,
                 SMC = TRUE,
                 cor = "cor")


# matrix of factor loadings
round(efa[["loadings"]], digits = 2)
```

The output of the `fa()` function is very detailed, potentially overwhelming, so we won't be showing it in the whole. Instead we will focus on the matrix of factor loadings. 

By looking at the matrix we can see that items `x1-x3` have high loadings on the factor `PA3`, items `x4-x6` on the factor `PA1` and `x7-x9` on `PA2`. We will use this insight to specify our confirmatory model. 

### Confirmatory Factor Analysis (CFA) 
To estimate the confirmatory factor analytic model, we will use the `lavaan` package. We specify three models:

1. Unidimensional model (one-factor model);
2. Three-dimensional congeneric model;
3. Three-dimensional tau-equivalent model. 

While it often makes sense to compare models 1 and 2 becasue model 1 is what we usually think of as a more parsimonious model (having fewer latent factors), it might be not clear why we would estimate models 2 and 3, and what does the terms "congeneric" and "tau-equivalent" even mean. In the congeneric model, we assume that the indicators measure the same construct but not necessairly to the same degree. With the tau-equivalent model, we assume that the indiactor measure the construct to the same degree and we do so by fixing unstandardized factor loadings to 1.0. If the fit of the latter is not substantially worse than the former, we can conclude that the indicators are tau-equivalent [@kline_principles_2016]. One of the big advantages of the tau-equivalent model is that it allows for a greater comparability of scores across independent studies that use the same items as the scores do not depend on the study-specifc factor loadings [@widaman_thinking_2023]. Also the correlation between the sum scores and the the true, or factor, scores is then high.

To estimate the three models with first define the model syntax. Then we choose specify the estimator (Robust Maximum Likelihood), and we set the option `std.lv = TRUE` to impose the identification constraints on the model - mean of the latent variable equal to 0 and the variance to 1.

```{r}
# Define models
one_factor_model <- '
g_factor =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9
'

three_factor_model <- '
visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9
'

three_factor_tau_model <- '
visual =~ a*x1 + a*x2 + a*x3
textual =~ b*x4 + b*x5 + b*x6
speed =~ c*x7 + c*x8 + c*x9
'

# Estimate cfa
# One factor model
one_factor_cfa <- lavaan::cfa(model = one_factor_model,
                              data = Data,
                              estimator = 'mlr',
                              std.lv = TRUE)
# Three factor model
three_factor_cfa <- lavaan::cfa(model = three_factor_model,
                                data = Data,
                                estimator = 'mlr',
                                std.lv = TRUE)

# Three factor model model with essential tau equivalence
three_factor_tau_cfa <- lavaan::cfa(model = three_factor_tau_model,
                                    data = Data,
                                    estimator = 'mlr',
                                    std.lv = TRUE)
```
::: {.column-margin}
For categorical data, we would change the `cfa()` call and we would specify, which variables should by treated as categorical by using the `ordered` argument. `lavaan` then automatically would switch to an appropriate estimator --- diagonally weighted least squares.  
:::

Once the models are estimated and no warning messages are shown, we can inspect the global fit of the models. 

```{r}
# model fit
# Define fit measures of interest
# use robust versions
fit_measures <- c("chisq.scaled", "df", "pvalue.scaled",
                  "cfi.robust", "rmsea.robust", "srmr",
                  "aic", "bic", "bic2")

# Extract model fit
round(lavaan::fitMeasures(one_factor_cfa, fit.measures = fit_measures), digits = 3)
round(lavaan::fitMeasures(three_factor_cfa, fit.measures = fit_measures), digits = 3)
round(lavaan::fitMeasures(three_factor_tau_cfa, fit.measures = fit_measures), digits = 3)
```
First, we inspect the scaled chi-square and the corresponding p-values. They suggest that our models fail the exact-fit test and that they do not fit the data well. 

We also check the approximate fit indices, i.e., the robust versions of the Comparative Fit Index (CFI) and the Root Mean Square Error of Approximation (RMSEA), and the Standardized Root Mean Square Residual (SRMR). There are different cut-off values proposed in the literature for those indices [e.g., @hu_cutoff_1999; @byrne_structural_1994]. We will assume that CFI values smaller than .950, RMSEA greater than .08, and SRMR greater than .10 suggest a poor fit. In the case of all of our models, the indices suggest unsatisfactory fit, with the three-factor model with varying loadings as the best-fitting. 

::: {.callout-important appearance="simple"}
## Be careful
Even though in this tutorial we follow the common practice of using fixed cut-off values for evaluating the model fit, this is not recommended by the current literature as the universally used cut-off values are based on simulation studies with a limited set of conditions which can substantially deviate from the ones that the researcher is facing [see, @groskurth_why_2023; @mcneish_dynamic_2023]. Ideally, researchers should derive the cut-offs using simulation-based techniques, which can done using for example the Shiny app developed by @mcneish_dynamic_2023 - https://dynamicfit.app/connect/
:::

We can use a statistical test to compare those models, i.e. the scaled chi-squared difference test. We exclude the one-factor model from the comparison since its fit is much worse than either of the three-dimensional models. To conduct the test, we use the `anova()` function. 

```{r}
anova(three_factor_cfa, three_factor_tau_cfa)
```
The test confirms the conclusions from the comparison of fit indices --- model with varying factor loadings fits the data the best.


To identify the problems with the best-fitting model, we will evaluate local model fit. Specifically, we will inspect the matrix of correlation residuals and we will look for residuals whose absolute value is greater than .10 as they can be suggestive of model misfit^[Evaluation of the local model fit often highlights the same model misspecifications as the inspection of modification indices. Yet, these two procedures have different premises. Inspection of correlations residuals address the question of the violation of the conditional independence assumption [indicators should be independent conditional on the latent variables, see @bollen_latent_2002]. With modification indices, we investigate factors that can improve model fit. We prefer the former as answering the question of independence assumption is more meaningful than merely improving the fit of the model.].


```{r}
# Evaluate local model fit
# Extract residual correlaton matrix
lavaan::lavResiduals(three_factor_cfa)$cov
```
The inspection of the residuals reveals that there are five residuals greater than .10, which suggest the violation of conditional independence assumption between those pairs of indicators:

-   `x1` with `x7` and `x9`;
-   `x2` with `x7`;
-   `x3` with `x5` and `x9`.

In a real-world application we would know more about the items than just a few keywords that we get with the `HolzingerSwineford1939` dataset. In any case, we see that our model generally fails to account for the observed correlations between the items belonging to the factor `visual` and those belonging to the factor `speed`, and for the observed association between items `x3` and `x5`. If we believed that the model misses meaningful associations between items and non-target constructs represented by the latent factors, we might specify cross-loadings. If we believed that the model fails to account for the shared sources of influence on the indicators that are unrelated to the factors, such as wording effects or context, we would specify residual covariances [@asparouhov_bayesian_2015]. In either case, we should explain the decision. Here, we will just specify the covariances.


```{r}
# Define models
three_factor_model <- '
visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9

x1~~x7
x1~~x9
x2~~x7
x3~~x5
x3~~x9
'

three_factor_cfa_res <- lavaan::cfa(model = three_factor_model,
                                data = Data,
                                estimator = 'mlr',
                                std.lv = TRUE)

round(lavaan::fitMeasures(three_factor_cfa_res, fit.measures = fit_measures), digits = 3)

lavaan::lavResiduals(three_factor_cfa_res)$cov
```
After we introduced the residual covariances, the model still fails to pass the exact-fit test, but has acceptable values on approximate fit indices and there are not other correlation residuals that would require our attention. We could check the value of the residual correlations (not correlation residuals!) by running `standardizedSolution(three_factor_cfa_res)` and subsetting the rows.

```{r}
standardized_solution <- standardizedSolution(three_factor_cfa_res)
subset(standardized_solution, grepl("~~", op) & 
         grepl("^x[0-9]+$", lhs) & 
         grepl("^x[0-9]+$", rhs) & 
         lhs != rhs)
```
::: {.column-margin}
We use `subset()` and `grepl()` to find rows, such that:

-   have `~~` in the column `op`
-   start with `x` followed by a numeric value in columns `lhs` and `rhs` 
-   and where the values for rows `lhs` and `rhs` are different (because we are interested in the correlations)    
:::
Now, we will inspect factor loadings and the correlation structure between the factors.
```{r}
lavaan::lavInspect(three_factor_cfa_res, "std")$lambda
# interfactor correlation
lavaan::lavInspect(three_factor_cfa_res, "std")$psi
```
We can see that all the standardized factor loadings have non-trivial (0.3<) values and vary in magnitude. Correlations between the factors range from 0.470 (`textual` and `visual`) to 0.278 (`speed` and `textual`). 

## Descriptive statistics of indicators, reliability, and criterion validity 
In this section we will take a closer look at the indicators themselves. First, we will compute descriptive statistics.

```{r}
psych::describe(Data[ , unlist(lookup_table$item)])[, c("mean", "sd", "skew", "kurtosis", "n")]
```

Then, we calculate the reliability coefficients. To learn about the reliability, scholars usually compute Cronbach’s α. Yet, this coefficient is not appropriate when the indicators are congeneric. If the factor loadings vary substantially, we should compute McDonald’s ω~h~  [@zinbarg_cronbachs_2005]. Still, we will compute both coeffcients for the sake of demonstration. For Cronbach’s α, we will compute the median and 95% credible interval.  

```{r}
#| message: false
# Estimate Cronbachs alpha as example for scale "textual"
textual_alphas <- psych::alpha(Data[, lookup_table[lookup_table$subscale == "textual", "item"]],
                               n.iter = 1000)

# Extract the median and the 95% CI
round(textual_alphas[["boot.ci"]], digits = 2) 

# McDonalds omega hierarchical
semTools::reliability(three_factor_cfa_res, what = "omega3")
```
The results suggest that the reliability for the scale "visual" is not satisfactory. 

Lastly, we will investigate the criterion validity. For this, we will compute correlations between the mean scale scores and four variables that we have in the dataset: gender, age, school, and grade.


```{r}
# Creating unweighted means 
for(subscale in unique(lookup_table$subscale)) {
  subscale_name <- paste(subscale, "mean", sep = "_")
  items <- lookup_table[lookup_table$subscale == subscale, "item"]
  mean_score <- rowMeans(Data[, items], na.rm = FALSE)
  Data[, subscale_name] <- mean_score
  }

# Transform school variable to a numeric variable
Data$school_numeric <- as.numeric(Data$school)
  

# Define variables for correlational analyses
cor_variables <- c("visual_mean", "textual_mean", "speed_mean",
                   "sex", "age", "school_numeric", "grade")

# Estimate correlations & p-values
cor_coef <- Hmisc::rcorr(as.matrix(Data[, cor_variables]))$r
cor_pval <- Hmisc::rcorr(as.matrix(Data[, cor_variables]))$P

# Filtering rows and columns
cor_coef <- cor_coef[!grepl("mean", rownames(cor_coef)),grepl("mean", colnames(cor_coef))]
cor_pval <- cor_pval[!grepl("mean", rownames(cor_pval)),grepl("mean", colnames(cor_pval))]

# Formatting the output to two and three decimal places
cor_coef <- as.data.frame(cor_coef) %>%
  mutate(across(everything(), ~ sprintf("%.2f", .)))
cor_pval <- as.data.frame(cor_pval) %>%
  mutate(across(everything(), ~ sprintf("%.3f", .)))

cor_coef
cor_pval 
```
Correlations are small to moderate and not all them are significant. We see that the scale visual correlates with gender and grade, scales textual and speed with age, school, and grade. 

## Descriptive statistics of the scales 
The very last part of this tutorial consists of computing descriptive statistics for the scale scores. 
```{r}
psych::describe(Data[, c("visual_mean", "textual_mean", "speed_mean")])[, c("mean", "sd", "skew", "kurtosis", "n")]
```


